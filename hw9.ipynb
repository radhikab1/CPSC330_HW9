{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251765cc",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"hw9.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33c5b0c-d0f1-40c0-b794-3b3471ac73d2",
   "metadata": {},
   "source": [
    "# CPSC 330 - Applied Machine Learning \n",
    "\n",
    "## Homework 9: Communication\n",
    "\n",
    "**Due date: See the [Calendar](https://htmlpreview.github.io/?https://github.com/UBC-CS/cpsc330/blob/master/docs/calendar.html)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086914c2-5de1-414a-8770-23bef9f312d0",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eced0368-de29-4ea7-9d71-7188d3749f5b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "## Instructions\n",
    "rubric={points}\n",
    "\n",
    "You will earn points for following these instructions and successfully submitting your work on Gradescope.  \n",
    "\n",
    "### Group work instructions\n",
    "\n",
    "**You may work with a partner on this homework and submit your assignment as a group.** Below are some instructions on working as a group.  \n",
    "\n",
    "- The maximum group size is 2.  \n",
    "- Use group work as an opportunity to collaborate and learn new things from each other. \n",
    "- Be respectful to each other and make sure you understand all the concepts in the assignment well. \n",
    "- It's your responsibility to make sure that the assignment is submitted by one of the group members before the deadline. \n",
    "- You can find the instructions on how to do group submission on Gradescope [here](https://help.gradescope.com/article/m5qz2xsnjy-student-add-group-members).\n",
    "- If you would like to use late tokens for the homework, all group members must have the necessary late tokens available. Please note that the late tokens will be counted for all members of the group.   \n",
    "\n",
    "\n",
    "### General submission instructions\n",
    "\n",
    "- Please **read carefully\n",
    "[Use of Generative AI policy](https://ubc-cs.github.io/cpsc330-2025W1/syllabus.html#use-of-generative-ai-in-the-course)** before starting the homework assignment. \n",
    "- **Run all cells before submitting:** Go to `Kernel -> Restart Kernel and Clear All Outputs`, then select `Run -> Run All Cells`. This ensures your notebook runs cleanly from start to finish without errors.\n",
    "  \n",
    "- **Submit your files on Gradescope.**  \n",
    "   - Upload only your `.ipynb` file **with outputs displayed** and any required output files.\n",
    "     \n",
    "   - Do **not** submit other files from your repository.  \n",
    "   - If you need help, see the [Gradescope Student Guide](https://lthub.ubc.ca/guides/gradescope-student-guide/).  \n",
    "- **Check that outputs render properly.**  \n",
    "   - Make sure all plots and outputs appear in your submission.\n",
    "     \n",
    "   - If your `.ipynb` file is too large and doesn't render on Gradescope, also upload a PDF or HTML version so the TAs can view your work.  \n",
    "- **Keep execution order clean.**  \n",
    "   - Execution numbers must start at \"1\" and increase in order.\n",
    "     \n",
    "   - Notebooks without visible outputs may not be graded.  \n",
    "   - Out-of-order or missing execution numbers may result in mark deductions.  \n",
    "- **Follow course submission guidelines:** Review the [CPSC 330 homework instructions](https://ubc-cs.github.io/cpsc330-2025W1/docs/homework_instructions.html) for detailed guidance on completing and submitting assignments. \n",
    "   \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69be5b2d-1854-4c63-bcc6-9b6258b7293a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3ebbf1-5b80-46d7-bd35-7e3a0c94324b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Exercise 1: Survival analysis\n",
    "<hr>\n",
    "\n",
    "rubric={points}\n",
    "\n",
    "The following questions pertain to the lecture on survival analysis. We'll consider the use case of customer churn analysis.\n",
    "\n",
    "1. What is the problem with simply labeling customers are \"churned\" or \"not churned\" and using standard supervised learning techniques?\n",
    "2. Consider customer A who just joined last week vs. customer B who has been with the service for a year. Who do you expect will leave the service first: probably customer A, probably customer B, or we don't have enough information to answer? Briefly explain your answer. \n",
    "3. If a customer's survival function is almost flat during a certain period, how do we interpret that?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d15ee3-57f3-4823-a7be-debedda8b3c9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10665422",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "1. The problem with labeling customers as ‚Äúchurned‚Äù or ‚Äúnot churned‚Äù is that this ignores time-to-event and fails to handle right-censoring. Many customers have not yet churned at the time we observe them, so their true churn time is censored, not ‚Äúnon-churn.‚Äù Treating these censored observations as if the event will never occur introduces bias, and removing them leaves you with only customers whose churn events have already happened (those with shorter observed event times). This distorts the distribution of churn times and leads standard supervised learning models to produce incorrect or misleading predictions.\n",
    "\n",
    "2. We cannot determine who is more likely to churn first, because survival analysis requires more than just the current observed time. The hazard of churn can increase, decrease, or stay constant over time depending on the service, and we have no information about the underlying hazard function or customer covariates. Therefore, based solely on ‚Äúone week vs. one year,‚Äù we do not have enough information to compare their time-to-event distributions or their hazards of churn.\n",
    "\n",
    "3. If the survival function is nearly flat during some time interval, it means that the probability of surviving beyond that time, i.e., not churning barely changes. In other words, very few churn events occur in that period, and the hazard of churn is low. Customers who reach that interval tend to continue remaining with the service, indicating a stable retention phase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dab5740-ae46-4efd-a08e-bf4ae9482701",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed594c68-91c3-45d9-baec-6ac38a02c971",
   "metadata": {},
   "source": [
    "## Exercise 2: Communication\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5767926d-c17d-4a93-b59a-7242a4c76ff0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.1 Blog post \n",
    "rubric={points}\n",
    "\n",
    "Write up your analysis from hw5 or any other assignment or your side project on machine learning in a \"blog post\" or report format. It's fine if you just write it here in this notebook. Alternatively, you can publish your blog post publicly and include a link here. (See exercise 4.3.) The target audience for your blog post is someone like yourself right before you took this course. They don't necessarily have ML knowledge, but they have a solid foundation in technical matters. The post should focus on explaining **your results and what you did** in a way that's understandable to such a person, **not** a lesson trying to teach someone about machine learning. Again: focus on the results and why they are interesting; avoid pedagogical content.\n",
    "\n",
    "Your post must include the following elements (not necessarily in this order):\n",
    "\n",
    "- Description of the problem/decision.\n",
    "- Description of the dataset (the raw data and/or some EDA).\n",
    "- Description of the model.\n",
    "- Description your results, both quantitatively and qualitatively. Make sure to refer to the original problem/decision.\n",
    "- A section on caveats, describing at least 3 reasons why your results might be incorrect, misleading, overconfident, or otherwise problematic. Make reference to your specific dataset, model, approach, etc. To check that your reasons are specific enough, make sure they would not make sense, if left unchanged, to most students' submissions; for example, do not just say \"overfitting\" without explaining why you might be worried about overfitting in your specific case.\n",
    "- At least 3 visualizations. These visualizations must be embedded/interwoven into the text, not pasted at the end. The text must refer directly to each visualization. For example \"as shown below\" or \"the figure demonstrates\" or \"take a look at Figure 1\", etc. It is **not** sufficient to put a visualization in without referring to it directly.\n",
    "\n",
    "A reasonable length for your entire post would be **800 words**. The maximum allowed is **1000 words**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6169eefb-18a8-4e13-b7ca-1b3539a79215",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Example blog posts\n",
    "\n",
    "Here are a few optional examples if you‚Äôd like inspiration. You don‚Äôt need to read them in full; instead, skim one and pay attention to:\n",
    "\n",
    "- how they introduce the question/problem,\n",
    "\n",
    "- how they describe the data and show a few key plots,\n",
    "\n",
    "- how they briefly describe the model without turning it into a tutorial, and\n",
    "\n",
    "- how they discuss results and limitations in plain language.\n",
    "\n",
    "**Examples**\n",
    "\n",
    "- [Will your Kickstarter Project be successful? | A simple analysis to help you predict better!!!](https://blog.goodaudience.com/kickstarter-projects-prediction-of-state-steps-for-a-beginner-analysis-f4630a50b7fe) (GoodAudience)\n",
    "- [My Little Markov Model - Now Tweeting New Taylor Swift Lyrics](https://ubc-mds.github.io/2022-12-01-my-little-markov-model/) (UBC MDS)\n",
    "- [What's for dinner? Predicting customer order probabilities](https://ubc-mds.github.io/2019-07-26-predicting-customer-probabilities/) (UBC MDS)\n",
    "\n",
    "**Additional inspiration**\n",
    "\n",
    "These are interviews with Kaggle competition winners. They aren‚Äôt blog posts, but they offer insight into how people articulate data, modeling decisions, and results:\n",
    "\n",
    "- [Instacart Market Basket Analysis](https://medium.com/kaggle-blog/instacart-market-basket-analysis-feda2700cded)\n",
    "- [Winner Interview with Shivam Bansal | Data Science for Good Challenge: City of Los Angeles](https://medium.com/kaggle-blog/winner-interview-with-shivam-bansal-data-science-for-good-challenge-city-of-los-angeles-3294c0ed1fb2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfdd094-eeb6-4f00-a3bc-5a6105eedb12",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### A note on plagiarism\n",
    "\n",
    "You may **NOT** include text or visualizations that were not written/created by you. If you are in any doubt as to what constitutes plagiarism, please just ask. For more information see the [UBC Academic Misconduct policies](https://academicintegrity.ubc.ca/regulation-process/academic-misconduct/). Please don't copy this from somewhere or ask Generative AI to write it for you üôè. Please carefully read [the Use of Generative AI policy](https://ubc-cs.github.io/cpsc330-2025W1/syllabus.html#use-of-generative-ai-in-the-course) before writing your blogpost. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a33abb-71b9-4308-b7a1-8a2d75a5afc6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2_1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 26"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa841dbc-517c-4642-aef5-7c0889790f2f",
   "metadata": {},
   "source": [
    "# Predicting Credit Card Default Probabilities\n",
    "\n",
    "Machine learning models can be very helpful in banking. They are good at recognizing patterns that a human might not, and they can perform certain tasks much faster than humans. Machine learning can improve efficacy and security in the banking field at low cost.\n",
    "\n",
    "Our task was to train a model to predict whether a client will default on their credit card payment. This is a **classificatio**n problem‚Äìwe are trying to predict which of the two **classes** (positive or negative) an example belongs to. To do this, we tried different models, each with their own strengths and weaknesses, then selected the best performing model at the end. \n",
    "\n",
    "Our process can be broken down into five steps:\n",
    "1. Wrangling and Exploratory Data Analysis (EDA)\n",
    "2. Feature Engineering and Preprocessing\n",
    "3. Modeling and Evaluation\n",
    "4. Interpretation\n",
    "5. Caveats and Limitations\n",
    "\n",
    "### Wrangling and EDA\n",
    "Our goal is to use training data to ‚Äúteach‚Äù a model the patterns associated with defaults. In order to use the data effectively and appropriately, we need to understand it very well. Fortunately, the data was already clean and tidy, meaning that we didn‚Äôt have to clean it ourselves. Our training data consists of 30,000 rows and 25 columns. Each row represents one customer and includes: \n",
    "\n",
    "Our **target** variable, the ‚Äúclass‚Äù of the observation:\n",
    "- default.payment.next.month\n",
    "And **features**, the explanatory variables:\n",
    "- Customer Id: 'ID'\n",
    "- Credit Limit: 'LIMIT_BAL'\n",
    "- Demographics:'SEX', 'EDUCATION','MARRIAGE','AGE'\n",
    "- Recent repayment statuses: 'PAY_0','PAY_2',...,'PAY_6'\n",
    "- Recent bill amounts: 'BILL_AMT1','BILL_AMT2',...,'BILL_AMT6'\n",
    "- Recent payment amounts: 'PAY_AMT1', 'PAY_AMT2',...,'PAY_AMT6'\n",
    "\n",
    "\n",
    "We took a quick look at the data by conducting some exploratory data analysis (EDA). We looked at summary statistics, data types, and some distributions.\n",
    "\n",
    "For example, we can look at the distributions of certain features: \n",
    "<div>\n",
    "<img src=\"../../images/balance_limit_dist\" width=\"500\"/>\n",
    "</div>\n",
    "<div>\n",
    "<img src=\"../../images/previous_amt_dist\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "We can even see the distribution according to class:\n",
    "<div>\n",
    "<img src=\"../../images/age_by_class\" width=\"500\"/>\n",
    "</div>\n",
    "Notably, for classification problems, we want to know if there is **class imbalance**‚Äìwhen one class is much more common than the other. If that is the case, we would need to take extra precautions when training our model. Thankfully, this chart shows us that while there is some imbalance, the issue is not severe.\n",
    "\n",
    "<div>\n",
    "<img src=\"../../images/class_imbalance\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "This helped us select ROC AUC score as our evaluation metric. Different scoring metrics have different uses. Given that we wanted to compare different models against each other, we chose ROC AUC because it provides a good summary of performance over all possible thresholds, and can address class imbalances.\n",
    "\n",
    "\n",
    "### Feature Engineering and Preprocessing\n",
    "\n",
    "We created some new features, in the hopes that they would either bring new insights or reduce computational burden. We averaged the different bill and payment amounts, then used those features to create an average ratio between the two. \n",
    "\n",
    "We then preprocessed the data, applying transformations so that the model is able to process them as inputs, they and that the real-world meaning of the data is preserved. \n",
    "\n",
    "We applied scaling to the numerical features to make sure that each feature has the same scale, thereby ensuring equal consideration for all features. We applied One Hot Encoding to the categorical features (sex, education, marriage). This is because we want to preserve their distinct meanings using a numerical format. \n",
    "\n",
    "### Models and Results\n",
    "We used a variety of models, each with its own strengths and weaknesses. \n",
    "\n",
    "\n",
    "| Model                        | How It Works |   CV ROC-AUC Score |   Fit Time | Comments                                                                                       |\n",
    "|:-----------------------------| ---| -------------:|-----------:|:--------------------------------------------------------------------------------------------|\n",
    "| Dummy                        | Guesses the most common class for every prediction |       0.5    |     0.0129 | A score of 0.5 means that it's as good as random. The dummy's purpose is to be used as a baseline, so it's okay that it does not perform well.                                                         |\n",
    "| Logistic Regression          |   Predicts the probability of the class based on linear coefficients of the input features |    0.7247 |     0.0763 | Significant improvement from Dummy, but still room for improvement                          |\n",
    "| Selected Logistic Regression |   Same as LR, but with only the features deemed most important|     0.7238 |     0.0342 |This model took in fewer features, limited to those deemed most important | 0.7238\t | It actually performed slightly worse than the normal Logistic Regression, indicating that the omitted features were informative.                |\n",
    "| Decision Tree                | Recursively splits the data into the classes based on feature thresholds|       0.6149 |     2.544  | Better performance than the dummy, but worse than Logistic Regression                                            |\n",
    "| Tuned Decision Tree          | Same as DT, but the max depth and class weight hyperparameters were tuned using GridSearch|        0.7538 |     0.801  | Significant improvement compared to un-tuned version, even better performance than LR.                                      |\n",
    "| Random Forest                | The average result of many DT's, each using bootstrapped samples and randomly split features. Generally better performing than a single DT due to reduction of variance  |     0.7633 |    25.963  | Good performance, but notably longer fit time                                               |\n",
    "| Tuned Random Forest          |  Same as RF, but with the best values of n_estimators, max depth, and max features found through RandomSearch. |    0.7788 |   117.108  | Slightly better performance compared to un-tuned version, but much longer fit time          |\n",
    "| Gradient Boosting            | Builds a series of weak models, with each model sequentially correcting the errors of the previous one. |     0.7792 |    41.515  | Best performance seen so far                                                                |\n",
    "| Tuned Gradient Boosting      | Same as GB, but with the best values of  n_estimators, max depth, and learning rate found through RandomSearch|     0.7778 |    85.951  | Notable increase in fit time, yet slightly decreased score compared to un-tuned version |\n",
    "\n",
    "\n",
    "\n",
    "### Interpretation\n",
    "It‚Äôs important to understand why a model arrived at a certain decision, especially in contexts such as banking, where decisions can impact customers‚Äô lives. \n",
    "\n",
    "We looked at how ‚Äúimportant‚Äù each feature was: \n",
    "Inset chart \n",
    "\n",
    "However, this chart does not tell us the direction of the influence. To look into the direction of influence, we can use SHAP values to construct a waterfall plot. This is a plot representing the decision behind one specific example: \n",
    "\n",
    "Insert image \n",
    "\n",
    "The red bars contribute toward a positive prediction, while the blue bars contribute to a negative one. \n",
    "\n",
    "### Caveats and Limitations\n",
    "\n",
    "#### Limited Tuning\n",
    "Because this was a school project done with time and resource constraints, we were not able to run a very robust search for hyperparameters. While we have some decent results, it‚Äôs very possible that there are better-performing hyperparameter values. This might mean that we are not using the best hyperparameters, or even the best model. \n",
    "\n",
    "\n",
    "\n",
    "#### Demographic Biases\n",
    "There was demographic information included in the features‚Äìsex, marriage, and education. Including these features might cause biases to show up in our model, which is particularly harmful for a context such as banking. If certain groups are systemically economically disadvantaged, that might show up in the data. If the model were actually used, it might display these biases. For example, the model might learn that young people with low education are at risk for default, and then penalize all young people with low education, even if they would not have defaulted. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4052395d-a695-4063-97b6-46c4e13016d8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59667c9-db6a-4c12-a556-5b9815ef3564",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.2 Effective communication technique\n",
    "rubric={points}\n",
    "\n",
    "Describe one effective communication technique that you used in your post, or an aspect of the post that you are particularly satisfied with. (Max 3 sentences.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea91029-9c18-4d0d-a916-78b391df9435",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2_2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ea9c37-34c9-4b3e-a2df-e00cedd3e8ae",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56965b8c-9f4d-4a68-be4c-2a745dcf9989",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### (optional, not for marks) 2.3\n",
    "\n",
    "Publish your blog post from 1.1 publicly using a tool like [Quarto](https://quarto.org/), or somewhere like medium.com, and paste a link here. Be sure to pick a tool in which code and code output look reasonable. This link could be a useful line on your resume!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21a1f7b-8904-4f84-b8e7-592041eec55b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2_3\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a55e63-38cb-4e36-867f-0261d1c583de",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cefc8e-cf76-4c27-9aa6-c560cbc0fc2b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Exercise 3: Your takeaway from the course \n",
    "rubric={points}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "- Reflect on your journey through this course. Please identify and elaborate on at least three key concepts or experiences where you had an \"aha\" moment. How would you use the concepts learned in this course in your personal projects or how would you approach your past projects differently based on the insights gained in this course? We encourage you to dig deep and share your genuine reflections.\n",
    "\n",
    "> Please write thoughtful answers. We are looking forward to reading them üôÇ. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99c85ae-6088-48cd-a23f-a7bc4e1b3f49",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_3\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fb9e2f-a2d2-4f56-9fb4-c91492e4801b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7e2e06-a835-4a5b-b572-daac1d73a171",
   "metadata": {},
   "source": [
    "As I look back on this course, there were a few moments that really changed how I think about machine learning. One major ‚Äúaha‚Äù moment was when I finally understood how important proper data splitting is. Before this course, I thought of train/validation/test splits as a routine step, but I didn‚Äôt realize how easy it is to accidentally leak information. Seeing how preprocessing the whole dataset before splitting or ignoring time order can completely ruin a model‚Äôs evaluation made me rethink a lot of my past work. If I revisited those projects now, I would definitely use proper pipelines and time-aware splits to make the results actually trustworthy.\n",
    "\n",
    "Another concept that was important for me was regularization and the bias‚Äìvariance tradeoff. It made me understand why high training accuracy isn't always preferrable. The examples in class showed how a model can look good on the surface while performing poorly in reality. It made me realize the value of slightly simpler, well-regularized models. Going forward, I‚Äôll pay a lot more attention to validation curves and generalization rather than just chasing the highest accuracy number.\n",
    "\n",
    "A third big moment came from learning about feature engineering and interpretability. I used to assume the model choice mattered more than the features, but the course made me see how much a thoughtful feature or a domain-specific transformation can improve performance. This is something I‚Äôll carry into future projects‚Äîspending more time understanding the data instead of jumping straight into complex models.\n",
    "\n",
    "Finally, survival analysis was completely new to me, and it changed how I think about problems like churn prediction. Learning about time-to-event modeling, hazard, and right-censoring made me realize how much more accurate and realistic survival methods are for this kind of problem, especially in past work.\n",
    "\n",
    "Overall, this course shifted my mindset. I now pay much more attention to data quality, evaluation, and assumptions instead of focusing only on algorithms. These lessons will definitely shape how I design future projects, taking use of better practices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4e160c-d947-4123-8e67-fa3c89c9aa8f",
   "metadata": {},
   "source": [
    "### Congratulations üëèüëè\n",
    "\n",
    "That's all for the assignments! Congratulations on finishing all homework assignments! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f3bee4-0171-4465-838f-e5ac8a943e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(\"img/eva-congrats.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef97272f-940a-4498-b3da-ec586c8407a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56061ab6-ad6e-4015-9a2d-0a4e02e719f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc930a0-a09c-48a5-83d3-24ddff8b8bb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniforge3]",
   "language": "python",
   "name": "conda-env-miniforge3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
